# L113 機器學習概念
- L11301 機器學習基本原理
- L11302 常見的機器學習模型

### L11301 機器學習基本原理
- 機器學習定義
- 類型  ==> 資料標籤 (Data Labeling / Data Annotation)
  - 監督式學習:  分類(辨識)   迴歸(預測)
  - 非監督式學習
  - 半監督式學習
  - 強化式學習
  - 比較分析==> 監督式學習 vs 非監督式學習
- 機器學習任務: == >  機器學習|評估指標 == > 產業應用
  - 迴歸(Regression) == >  迴歸(Regression)評估指標 
  - 分類(Tasks) == > 評估指標 
    - 二元分類(Binary)
    - 多元分類(Multi-Class)
    - 多標籤分類(Multi-label)  
  - 叢集(Cluster)
  - 降維(Dimensionality reduction)
  - 孤立子偵測|異常偵測(Outlier Detection| Anomaly Detection)
    - z 分數法（Z-Score Method）
    - IQR Method (Interquartile Range, 四分位距法)
    - 中階主題(機器學習與深度學習演算法)
      - Isolation Forest（孤立森林）
      - One-Class SVM（One-Class Support Vector Machine）
        - 只學習一個類別（通常是正常數據）的模式，並構建一個能將大多數正常樣本包圍起來的決策邊界。
        - 任何落在這個邊界之外的資料點都被判定為異常 
      - Local Outlier Factor (LOF, 2000): 非監督式演算法
  - 不平衡學習 (Imbalance Learning)
    - 欠採樣(Under-sampling) vs 過採樣(Over-sampling)
    - 過採樣(Over-sampling)
      - SMOTE (Synthetic Minority Oversampling Technique)
      - ADASYN(自適應合成取樣)2008
- 機器學習流程  Machine learning pipeline(管線)
  - 很多主題(中階重點)
  - 數據準備(資料處理)與特徵工程 ==>特徵工程 ==> 特徵選擇（Feature Selection）
  - 模型選擇與架構設計
  - 模型訓練、評估與驗證 ==>交叉驗證
  - 模型調整與優化 ==>

### L11302 常見的機器學習模型
- 監督式學習 : 迴歸(Regression)
  - 評估指標:
    - 平均平方誤差（MSE, Mean Squared Error）|平均絕對誤差（MAE, Mean Absolute Error）
    - 決定係數 𝑅2 | 調整後 𝑅2
  - 傳統迴歸 ==> (中階主題)適用情境 與使用限制
    - 線性迴歸(Linear Regression)
    - 多元線性迴歸(Multiple Linear Regression)
    - 多項式迴歸(Polynomial Regression)
    - 基本假設(中階主題):==> 更進階主題:如何檢定底下假設成立
      - `１`.線性關係（Linearity）:自變數與應變數之間應存在線性關係。
      - `2`.誤差常態分佈（Normality of Errors）==>誤差項ε 須符合常態分佈。
      - `3`.變異數齊一性（Homoscedasticity）==>誤差項的變異數應在不同自變數取值下保持相同。
      - `4`.誤差獨立性（Independence of Errors）==> 各觀測值之間的誤差應獨立無關。
      - `5`.無多重共線性（No Multicollinearity）==>自變數之間不應高度相關，以免影響係數估計的穩定性。
  - 正則化迴歸 ==> 希望防止模型過度擬合(overfitting)
    - 嶺迴歸(Ridge Regression)(L2)
    - 套索迴歸(Lasso Regression)(L1)
    - 彈性網迴歸(Elastic Net Regression)
  - 其他 (中階主題)==>  神經網路(Neural Network),決策樹迴歸 ,支援向量機迴歸, 集成學習迴歸
- 監督式學習 : 分類(Tasks) 
  - 評估指標 Accuracy（準確率）|Precision（精確率）|Recall（召回率）|F1-score | ROC-AUC
    - 考計算題 
  - 單一演算法
    - Logistic Regression(羅吉斯迴歸)
    - K-近鄰演算法(K-nearest neighbors, KNN)
    - 決策樹(Decision Tree)
    - 支援向量機 SVM (Support Vector Machine)
    - 樸素貝式分類（Naïve Bayes Classifier）
    - 神經網路(Neural Network)
  - 集成學習(Ensemble Learning)
    - 集成投票分類(Ensemble Voting) ==> 軟投票(Soft voting) 硬投票(Hard voting) 加權軟投票()
    - 自助聚合法(Bootstrap Aggregation) Bagging ==> 隨機森林(Random forest)
    - 提升法(Boosting) ==> 適應提升(Adaptive Boosting, AdaBoost)  梯度提升(Gradient Boosting)
    - 堆疊法(Stacking)
    - 混合集成(Blending)
- 非監督式學習(1) :叢集(Cluster)
  - 核心關鍵:相似度(Similarity) ==>群組`內`相似度高 群組`間`相似度低 ==>  評估指標: Silhouette score（輪廓係數）
  - 叢集(Cluster):四大類型(沒有客觀上「正確」的叢集演算法分類,有人增加網格叢集:STING 和 CLIQUE)
  - `1`.基於`質心(Center)`的方法 ==> K-Means 叢集
  - `2`.基於`分佈(Distribution)`的方法 ==>高斯混合模型
  - `3`.基於`連接(connection)`的方法 ==> 階層式叢集 | 聚集叢集 |分裂叢集
  - `4`.基於`密度(Density)`的方法==> DBSCAN（基於密度的雜訊應用空間叢集） 
- 非監督式學習(2) : 降維(Dimensionality reduction)
  - 線性降維方法
    - PCA(Principal Component Analysis)主成分分析
    - 線性判別分析（Linear Discriminant Analysis, LDA）(中階主題)
  - 非線性降維方法 (中階主題)
    - t-SNE（t-distributed Stochastic Neighbor Embedding）
    - UMAP（Uniform Manifold Approximation and Projection） 
  - (中階主題)奇異值分解（Singular Value Decomposition, SVD）矩陣分解技術
- 非監督式學習(3)(中階主題) : 關聯規則學習（Association Rule Learning）
  - 從大型資料集中發現不同項目之間有趣的、非平凡的關係或關聯。
  - 最典型的應用是「購物籃分析」，透過分析顧客的購買行為，找出哪些商品經常被一起購買 ==>  尿布與啤酒的故事
  - Apriori 演算法(先驅論文)
  - FP-Growth(比Apriori 更高效的關聯規則挖掘演算法)
 
