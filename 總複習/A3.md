# L113 æ©Ÿå™¨å­¸ç¿’æ¦‚å¿µ
- L11301 æ©Ÿå™¨å­¸ç¿’åŸºæœ¬åŸç†
- L11302 å¸¸è¦‹çš„æ©Ÿå™¨å­¸ç¿’æ¨¡å‹

### L11301 æ©Ÿå™¨å­¸ç¿’åŸºæœ¬åŸç†
- æ©Ÿå™¨å­¸ç¿’å®šç¾©
- é¡å‹  ==> è³‡æ–™æ¨™ç±¤ (Data Labeling / Data Annotation)
  - ç›£ç£å¼å­¸ç¿’:  åˆ†é¡(è¾¨è­˜)   è¿´æ­¸(é æ¸¬)
  - éç›£ç£å¼å­¸ç¿’
  - åŠç›£ç£å¼å­¸ç¿’
  - å¼·åŒ–å¼å­¸ç¿’
  - æ¯”è¼ƒåˆ†æ==> ç›£ç£å¼å­¸ç¿’ vs éç›£ç£å¼å­¸ç¿’
- æ©Ÿå™¨å­¸ç¿’ä»»å‹™: == >  æ©Ÿå™¨å­¸ç¿’|è©•ä¼°æŒ‡æ¨™ == > ç”¢æ¥­æ‡‰ç”¨
  - è¿´æ­¸(Regression) == >  è¿´æ­¸(Regression)è©•ä¼°æŒ‡æ¨™ 
  - åˆ†é¡(Tasks) == > è©•ä¼°æŒ‡æ¨™ 
    - äºŒå…ƒåˆ†é¡(Binary)
    - å¤šå…ƒåˆ†é¡(Multi-Class)
    - å¤šæ¨™ç±¤åˆ†é¡(Multi-label)  
  - å¢é›†(Cluster)
  - é™ç¶­(Dimensionality reduction)
  - å­¤ç«‹å­åµæ¸¬|ç•°å¸¸åµæ¸¬(Outlier Detection| Anomaly Detection)
    - z åˆ†æ•¸æ³•ï¼ˆZ-Score Methodï¼‰
    - IQR Method (Interquartile Range, å››åˆ†ä½è·æ³•)
    - ä¸­éšä¸»é¡Œ(æ©Ÿå™¨å­¸ç¿’èˆ‡æ·±åº¦å­¸ç¿’æ¼”ç®—æ³•)
      - Isolation Forestï¼ˆå­¤ç«‹æ£®æ—ï¼‰
      - One-Class SVMï¼ˆOne-Class Support Vector Machineï¼‰
        - åªå­¸ç¿’ä¸€å€‹é¡åˆ¥ï¼ˆé€šå¸¸æ˜¯æ­£å¸¸æ•¸æ“šï¼‰çš„æ¨¡å¼ï¼Œä¸¦æ§‹å»ºä¸€å€‹èƒ½å°‡å¤§å¤šæ•¸æ­£å¸¸æ¨£æœ¬åŒ…åœèµ·ä¾†çš„æ±ºç­–é‚Šç•Œã€‚
        - ä»»ä½•è½åœ¨é€™å€‹é‚Šç•Œä¹‹å¤–çš„è³‡æ–™é»éƒ½è¢«åˆ¤å®šç‚ºç•°å¸¸ 
      - Local Outlier Factor (LOF, 2000): éç›£ç£å¼æ¼”ç®—æ³•
  - ä¸å¹³è¡¡å­¸ç¿’ (Imbalance Learning)
    - æ¬ æ¡æ¨£(Under-sampling) vs éæ¡æ¨£(Over-sampling)
    - éæ¡æ¨£(Over-sampling)
      - SMOTE (Synthetic Minority Oversampling Technique)
      - ADASYN(è‡ªé©æ‡‰åˆæˆå–æ¨£)2008
- æ©Ÿå™¨å­¸ç¿’æµç¨‹  Machine learning pipeline(ç®¡ç·š)
  - å¾ˆå¤šä¸»é¡Œ(ä¸­éšé‡é»)
  - æ•¸æ“šæº–å‚™(è³‡æ–™è™•ç†)èˆ‡ç‰¹å¾µå·¥ç¨‹ ==>ç‰¹å¾µå·¥ç¨‹ ==> ç‰¹å¾µé¸æ“‡ï¼ˆFeature Selectionï¼‰
  - æ¨¡å‹é¸æ“‡èˆ‡æ¶æ§‹è¨­è¨ˆ
  - æ¨¡å‹è¨“ç·´ã€è©•ä¼°èˆ‡é©—è­‰ ==>äº¤å‰é©—è­‰
  - æ¨¡å‹èª¿æ•´èˆ‡å„ªåŒ– ==>

### L11302 å¸¸è¦‹çš„æ©Ÿå™¨å­¸ç¿’æ¨¡å‹
- ç›£ç£å¼å­¸ç¿’ : è¿´æ­¸(Regression)
  - è©•ä¼°æŒ‡æ¨™:
    - å¹³å‡å¹³æ–¹èª¤å·®ï¼ˆMSE, Mean Squared Errorï¼‰|å¹³å‡çµ•å°èª¤å·®ï¼ˆMAE, Mean Absolute Errorï¼‰
    - æ±ºå®šä¿‚æ•¸ ğ‘…2 | èª¿æ•´å¾Œ ğ‘…2
  - å‚³çµ±è¿´æ­¸ ==> (ä¸­éšä¸»é¡Œ)é©ç”¨æƒ…å¢ƒ èˆ‡ä½¿ç”¨é™åˆ¶
    - ç·šæ€§è¿´æ­¸(Linear Regression)
    - å¤šå…ƒç·šæ€§è¿´æ­¸(Multiple Linear Regression)
    - å¤šé …å¼è¿´æ­¸(Polynomial Regression)
    - åŸºæœ¬å‡è¨­(ä¸­éšä¸»é¡Œ):==> æ›´é€²éšä¸»é¡Œ:å¦‚ä½•æª¢å®šåº•ä¸‹å‡è¨­æˆç«‹
      - `ï¼‘`.ç·šæ€§é—œä¿‚ï¼ˆLinearityï¼‰:è‡ªè®Šæ•¸èˆ‡æ‡‰è®Šæ•¸ä¹‹é–“æ‡‰å­˜åœ¨ç·šæ€§é—œä¿‚ã€‚
      - `2`.èª¤å·®å¸¸æ…‹åˆ†ä½ˆï¼ˆNormality of Errorsï¼‰==>èª¤å·®é …Îµ é ˆç¬¦åˆå¸¸æ…‹åˆ†ä½ˆã€‚
      - `3`.è®Šç•°æ•¸é½Šä¸€æ€§ï¼ˆHomoscedasticityï¼‰==>èª¤å·®é …çš„è®Šç•°æ•¸æ‡‰åœ¨ä¸åŒè‡ªè®Šæ•¸å–å€¼ä¸‹ä¿æŒç›¸åŒã€‚
      - `4`.èª¤å·®ç¨ç«‹æ€§ï¼ˆIndependence of Errorsï¼‰==> å„è§€æ¸¬å€¼ä¹‹é–“çš„èª¤å·®æ‡‰ç¨ç«‹ç„¡é—œã€‚
      - `5`.ç„¡å¤šé‡å…±ç·šæ€§ï¼ˆNo Multicollinearityï¼‰==>è‡ªè®Šæ•¸ä¹‹é–“ä¸æ‡‰é«˜åº¦ç›¸é—œï¼Œä»¥å…å½±éŸ¿ä¿‚æ•¸ä¼°è¨ˆçš„ç©©å®šæ€§ã€‚
  - æ­£å‰‡åŒ–è¿´æ­¸ ==> å¸Œæœ›é˜²æ­¢æ¨¡å‹éåº¦æ“¬åˆ(overfitting)
    - å¶ºè¿´æ­¸(Ridge Regression)(L2)
    - å¥—ç´¢è¿´æ­¸(Lasso Regression)(L1)
    - å½ˆæ€§ç¶²è¿´æ­¸(Elastic Net Regression)
  - å…¶ä»– (ä¸­éšä¸»é¡Œ)==>  ç¥ç¶“ç¶²è·¯(Neural Network),æ±ºç­–æ¨¹è¿´æ­¸ ,æ”¯æ´å‘é‡æ©Ÿè¿´æ­¸, é›†æˆå­¸ç¿’è¿´æ­¸
- ç›£ç£å¼å­¸ç¿’ : åˆ†é¡(Tasks) 
  - è©•ä¼°æŒ‡æ¨™ Accuracyï¼ˆæº–ç¢ºç‡ï¼‰|Precisionï¼ˆç²¾ç¢ºç‡ï¼‰|Recallï¼ˆå¬å›ç‡ï¼‰|F1-score | ROC-AUC
    - è€ƒè¨ˆç®—é¡Œ 
  - å–®ä¸€æ¼”ç®—æ³•
    - Logistic Regression(ç¾…å‰æ–¯è¿´æ­¸)
    - K-è¿‘é„°æ¼”ç®—æ³•(K-nearest neighbors, KNN)
    - æ±ºç­–æ¨¹(Decision Tree)
    - æ”¯æ´å‘é‡æ©Ÿ SVM (Support Vector Machine)
    - æ¨¸ç´ è²å¼åˆ†é¡ï¼ˆNaÃ¯ve Bayes Classifierï¼‰
    - ç¥ç¶“ç¶²è·¯(Neural Network)
  - é›†æˆå­¸ç¿’(Ensemble Learning)
    - é›†æˆæŠ•ç¥¨åˆ†é¡(Ensemble Voting) ==> è»ŸæŠ•ç¥¨(Soft voting) ç¡¬æŠ•ç¥¨(Hard voting) åŠ æ¬Šè»ŸæŠ•ç¥¨()
    - è‡ªåŠ©èšåˆæ³•(Bootstrap Aggregation) Bagging ==> éš¨æ©Ÿæ£®æ—(Random forest)
    - æå‡æ³•(Boosting) ==> é©æ‡‰æå‡(Adaptive Boosting, AdaBoost)  æ¢¯åº¦æå‡(Gradient Boosting)
    - å †ç–Šæ³•(Stacking)
    - æ··åˆé›†æˆ(Blending)
- éç›£ç£å¼å­¸ç¿’(1) :å¢é›†(Cluster)
  - æ ¸å¿ƒé—œéµ:ç›¸ä¼¼åº¦(Similarity) ==>ç¾¤çµ„`å…§`ç›¸ä¼¼åº¦é«˜ ç¾¤çµ„`é–“`ç›¸ä¼¼åº¦ä½ ==>  è©•ä¼°æŒ‡æ¨™: Silhouette scoreï¼ˆè¼ªå»“ä¿‚æ•¸ï¼‰
  - å¢é›†(Cluster):å››å¤§é¡å‹(æ²’æœ‰å®¢è§€ä¸Šã€Œæ­£ç¢ºã€çš„å¢é›†æ¼”ç®—æ³•åˆ†é¡,æœ‰äººå¢åŠ ç¶²æ ¼å¢é›†:STING å’Œ CLIQUE)
  - `1`.åŸºæ–¼`è³ªå¿ƒ(Center)`çš„æ–¹æ³• ==> K-Means å¢é›†
  - `2`.åŸºæ–¼`åˆ†ä½ˆ(Distribution)`çš„æ–¹æ³• ==>é«˜æ–¯æ··åˆæ¨¡å‹
  - `3`.åŸºæ–¼`é€£æ¥(connection)`çš„æ–¹æ³• ==> éšå±¤å¼å¢é›† | èšé›†å¢é›† |åˆ†è£‚å¢é›†
  - `4`.åŸºæ–¼`å¯†åº¦(Density)`çš„æ–¹æ³•==> DBSCANï¼ˆåŸºæ–¼å¯†åº¦çš„é›œè¨Šæ‡‰ç”¨ç©ºé–“å¢é›†ï¼‰ 
- éç›£ç£å¼å­¸ç¿’(2) : é™ç¶­(Dimensionality reduction)
  - ç·šæ€§é™ç¶­æ–¹æ³•
    - PCA(Principal Component Analysis)ä¸»æˆåˆ†åˆ†æ
    - ç·šæ€§åˆ¤åˆ¥åˆ†æï¼ˆLinear Discriminant Analysis, LDAï¼‰(ä¸­éšä¸»é¡Œ)
  - éç·šæ€§é™ç¶­æ–¹æ³• (ä¸­éšä¸»é¡Œ)
    - t-SNEï¼ˆt-distributed Stochastic Neighbor Embeddingï¼‰
    - UMAPï¼ˆUniform Manifold Approximation and Projectionï¼‰ 
  - (ä¸­éšä¸»é¡Œ)å¥‡ç•°å€¼åˆ†è§£ï¼ˆSingular Value Decomposition, SVDï¼‰çŸ©é™£åˆ†è§£æŠ€è¡“
- éç›£ç£å¼å­¸ç¿’(3)(ä¸­éšä¸»é¡Œ) : é—œè¯è¦å‰‡å­¸ç¿’ï¼ˆAssociation Rule Learningï¼‰
  - å¾å¤§å‹è³‡æ–™é›†ä¸­ç™¼ç¾ä¸åŒé …ç›®ä¹‹é–“æœ‰è¶£çš„ã€éå¹³å‡¡çš„é—œä¿‚æˆ–é—œè¯ã€‚
  - æœ€å…¸å‹çš„æ‡‰ç”¨æ˜¯ã€Œè³¼ç‰©ç±ƒåˆ†æã€ï¼Œé€éåˆ†æé¡§å®¢çš„è³¼è²·è¡Œç‚ºï¼Œæ‰¾å‡ºå“ªäº›å•†å“ç¶“å¸¸è¢«ä¸€èµ·è³¼è²· ==>  å°¿å¸ƒèˆ‡å•¤é…’çš„æ•…äº‹
  - Apriori æ¼”ç®—æ³•(å…ˆé©…è«–æ–‡)
  - FP-Growth(æ¯”Apriori æ›´é«˜æ•ˆçš„é—œè¯è¦å‰‡æŒ–æ˜æ¼”ç®—æ³•)


 ### ğŸ§  ç¬¬ 1 ç« ï¼šç·šæ€§è¿´æ­¸åŸºæœ¬æ¦‚å¿µ

ç·šæ€§è¿´æ­¸æ˜¯ä¸€ç¨®**ç›£ç£å¼å­¸ç¿’ (Supervised Learning)** æ–¹æ³•ï¼Œç”¨æ–¼**é æ¸¬é€£çºŒå‹æ•¸å€¼**ã€‚  
æ ¸å¿ƒå‡è¨­æ˜¯ã€Œè‡ªè®Šæ•¸èˆ‡æ‡‰è®Šæ•¸ä¹‹é–“å­˜åœ¨ç·šæ€§é—œä¿‚ã€ã€‚

### âœ³ï¸ åŸºæœ¬å½¢å¼

$$
y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_nx_n + \epsilon
$$

å…¶ä¸­ï¼š
- \( y \)ï¼šç›®æ¨™è®Šæ•¸ï¼ˆDependent Variableï¼‰  
- \( x_i \)ï¼šè‡ªè®Šæ•¸ï¼ˆIndependent Variableï¼‰  
- \( \beta_i \)ï¼šå›æ­¸ä¿‚æ•¸ï¼ˆRegression Coefficientsï¼‰  
- \( \epsilon \)ï¼šèª¤å·®é …ï¼ˆError Termï¼‰

---

## ğŸ“Š ç¬¬ 2 ç« ï¼šæ•¸å­¸æ¨å°èˆ‡æœ€å°å¹³æ–¹æ³• (OLS)

ç·šæ€§è¿´æ­¸çš„æ ¸å¿ƒæ˜¯æœ€å°åŒ–æ®˜å·®å¹³æ–¹å’Œï¼ˆSum of Squared Errors, SSEï¼‰ï¼š

$$
SSE = \sum_{i=1}^{m}(y_i - \hat{y}_i)^2 = \sum_{i=1}^{m}(y_i - X_i \beta)^2
$$

**æœ€å°å¹³æ–¹æ³• (Ordinary Least Squares, OLS)** çš„é–‰åˆè§£ï¼š

$$
\hat{\beta} = (X^TX)^{-1}X^Ty
$$

é€™å€‹å…¬å¼å¯ç›´æ¥æ±‚å¾—æœ€ä½³ä¿‚æ•¸ï¼Œä½¿æ¨¡å‹çš„é æ¸¬èª¤å·®æœ€å°åŒ–ã€‚

