# ä½¿ç”¨ Transformer åˆ†æ IMDB é›»å½±è©•è«–æƒ…æ„Ÿï¼ˆå«æ—©åœæ©Ÿåˆ¶ï¼‰

> ç›®æ¨™ï¼šä»¥ **Hugging Face Transformers**ï¼ˆå¦‚ DistilBERT/BERTï¼‰å¾®èª¿äºŒå…ƒæƒ…æ„Ÿåˆ†é¡å™¨ï¼Œä¸¦åœ¨ **IMDB** å½±è©•è³‡æ–™é›†ä¸Šè©•ä¼° Accuracy/F1ã€æ··æ·†çŸ©é™£èˆ‡éŒ¯èª¤åˆ†æï¼Œ**åŠ å…¥æ—©åœï¼ˆEarly Stoppingï¼‰æ©Ÿåˆ¶** æå‡è¨“ç·´ç©©å®šæ€§èˆ‡é˜²æ­¢éæ“¬åˆã€‚

---

## ç›®éŒ„

* [ç’°å¢ƒéœ€æ±‚èˆ‡å®‰è£](#ç’°å¢ƒéœ€æ±‚èˆ‡å®‰è£)
* [è³‡æ–™èˆ‡æ–¹æ³•ç¸½è¦½](#è³‡æ–™èˆ‡æ–¹æ³•ç¸½è¦½)
* [å¯¦ä½œæ­¥é©Ÿï¼ˆå« Early Stoppingï¼‰](#å¯¦ä½œæ­¥é©Ÿå«-early-stopping)

  * [1. è®€å–è³‡æ–™ï¼ˆ`datasets` çš„ `imdb`ï¼‰](#1-è®€å–è³‡æ–™datasets-çš„-imdb)
  * [2. æ¨™è¨˜åŒ–ï¼ˆTokenizerï¼‰èˆ‡è³‡æ–™é›†è½‰æ›](#2-æ¨™è¨˜åŒ–tokenizerèˆ‡è³‡æ–™é›†è½‰æ›)
  * [3. å»ºæ¨¡ï¼ˆ`AutoModelForSequenceClassification`ï¼‰](#3-å»ºæ¨¡automodelforsequenceclassification)
  * [4. è¨“ç·´è¨­å®šï¼ˆå« EarlyStoppingCallbackï¼‰](#4-è¨“ç·´è¨­å®šå«-earlystoppingcallback)
  * [5. è¨“ç·´èˆ‡é©—è­‰ã€æ··æ·†çŸ©é™£](#5-è¨“ç·´èˆ‡é©—è­‰æ··æ·†çŸ©é™£)
  * [6. éŒ¯èª¤åˆ†æï¼ˆTop èª¤åˆ¤ï¼‰](#6-éŒ¯èª¤åˆ†ætop-èª¤åˆ¤)
  * [7. æ¨¡å‹ä¿å­˜ã€è¼‰å…¥èˆ‡æ¨è«–](#7-æ¨¡å‹ä¿å­˜è¼‰å…¥èˆ‡æ¨è«–)
* [å»¶ä¼¸ï¼šæ³¨æ„åŠ›è¦–è¦ºåŒ–èˆ‡ Grad-CAM](#å»¶ä¼¸æ³¨æ„åŠ›è¦–è¦ºåŒ–èˆ‡-grad-cam)
* [å®Œæ•´ç¨‹å¼ç¢¼](#å®Œæ•´ç¨‹å¼ç¢¼)

---

## 4. è¨“ç·´è¨­å®šï¼ˆå« EarlyStoppingCallbackï¼‰

Transformers æä¾› **`EarlyStoppingCallback`**ï¼Œå¯åœ¨é©—è­‰é›†æŒ‡æ¨™ï¼ˆå¦‚ F1ï¼‰æ–¼å¤šæ¬¡è©•ä¼°å¾Œç„¡æ˜é¡¯æå‡æ™‚ï¼Œè‡ªå‹•åœæ­¢è¨“ç·´ã€‚

ä¸»è¦åƒæ•¸ï¼š

* `early_stopping_patience`: å®¹å¿ epoch æ•¸ï¼ˆå¦‚ 2 â†’ è‹¥é€£çºŒ 2 æ¬¡ç„¡æå‡å‰‡åœæ­¢ï¼‰
* `early_stopping_threshold`: æå‡é–¾å€¼ï¼ˆå°æ–¼æ­¤å€¼è¦–ç‚ºç„¡é€²æ­¥ï¼‰
* éœ€è¨­å®š `load_best_model_at_end=True` ä¸¦æŒ‡å®š `metric_for_best_model`

---

## å®Œæ•´ç¨‹å¼ç¢¼ï¼ˆå«æ—©åœï¼‰

```python
from __future__ import annotations
import numpy as np
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
    EarlyStoppingCallback
)
import evaluate
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

MODEL_NAME = "distilbert-base-uncased"
MAX_LEN = 256
EPOCHS = 6
SEED = 42
BATCH_TRAIN = 16
BATCH_EVAL = 32

# å›ºå®šéš¨æ©Ÿç¨®å­
np.random.seed(SEED)
torch.manual_seed(SEED)

# === 1. è¼‰å…¥ IMDB ===
print("[INFO] Loading IMDB dataset ...")
raw_ds = load_dataset("imdb")
train_valid = raw_ds["train"].train_test_split(test_size=0.1, seed=SEED)
train_ds, valid_ds, test_ds = train_valid["train"], train_valid["test"], raw_ds["test"]

# === 2. Tokenizer ===
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def tokenize_fn(batch):
    return tokenizer(batch["text"], truncation=True, max_length=MAX_LEN)

train_enc = train_ds.map(tokenize_fn, batched=True, remove_columns=["text"])
valid_enc = valid_ds.map(tokenize_fn, batched=True, remove_columns=["text"])
test_enc  = test_ds.map(tokenize_fn,  batched=True, remove_columns=["text"])

train_enc.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
valid_enc.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
test_enc.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

# === 3. æ¨¡å‹ ===
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME, num_labels=2, id2label={0: 'NEG', 1: 'POS'}, label2id={'NEG':0,'POS':1}
)

# === 4. è©•ä¼°èˆ‡ Early Stopping ===
acc_metric = evaluate.load("accuracy")
f1_metric  = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    acc = acc_metric.compute(predictions=preds, references=labels)["accuracy"]
    f1  = f1_metric.compute(predictions=preds, references=labels, average="binary")["f1"]
    return {"accuracy": acc, "f1": f1}

args = TrainingArguments(
    output_dir="./imdb_bert_earlystop",
    learning_rate=2e-5,
    per_device_train_batch_size=BATCH_TRAIN,
    per_device_eval_batch_size=BATCH_EVAL,
    num_train_epochs=EPOCHS,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    greater_is_better=True,
    fp16=torch.cuda.is_available(),
    seed=SEED,
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_enc,
    eval_dataset=valid_enc,
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer),
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.0001)]
)

# === 5. è¨“ç·´èˆ‡é©—è­‰ ===
trainer.train()

# === 6. æ¸¬è©¦é›†è©•ä¼° ===
raw_preds = trainer.predict(test_enc)
labels = raw_preds.label_ids
preds = np.argmax(raw_preds.predictions, axis=-1)
print(classification_report(labels, preds, target_names=['NEG','POS']))

cm = confusion_matrix(labels, preds)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['NEG','POS'], yticklabels=['NEG','POS'])
plt.title('Confusion Matrix with Early Stopping')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# === 7. ä¿å­˜æ¨¡å‹ ===
trainer.save_model("./imdb_bert_earlystop_model")
tokenizer.save_pretrained("./imdb_bert_earlystop_model")
```

---

### ğŸ“˜ Early Stopping å·¥ä½œåŸç†

* æ¯å€‹ epoch çµæŸå¾ŒåŸ·è¡Œè©•ä¼°ã€‚
* è‹¥ F1 åˆ†æ•¸é€£çºŒå…©æ¬¡æœªæ˜é¡¯æå‡ï¼ˆå·®è· < 0.0001ï¼‰ï¼Œå‰‡æå‰çµæŸè¨“ç·´ã€‚
* `load_best_model_at_end=True` æœƒè‡ªå‹•è¼‰å…¥æœ€ä½³æ¨¡å‹æ¬Šé‡ï¼ˆä¾æ“š metric_for_best_modelï¼‰ã€‚

---

## å°çµ

æ—©åœï¼ˆEarly Stoppingï¼‰èƒ½æœ‰æ•ˆé˜²æ­¢éæ“¬åˆï¼Œç¯€çœè¨“ç·´æ™‚é–“ï¼ŒåŒæ™‚ä¿æŒæœ€ä½³æ³›åŒ–æ€§èƒ½ã€‚çµåˆ **Transformer æ¨¡å‹å¾®èª¿**ï¼Œå¯åœ¨ IMDB æƒ…æ„Ÿåˆ†æä¸­é”åˆ°é«˜æº–ç¢ºç‡èˆ‡ç©©å®šè¨“ç·´è¡¨ç¾ã€‚
