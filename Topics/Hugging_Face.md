## **Hugging Face** 
> Hugging Face ç”Ÿæ…‹ç³» æ˜¯ AI æ¨¡å‹é–‹ç™¼çš„æ¨™æº–å¹³å°ï¼Œ
> å®ƒå°‡ è³‡æ–™ï¼ˆdatasetsï¼‰+ æ¨¡å‹ï¼ˆtransformersï¼‰+ å¾®èª¿ï¼ˆpeftï¼‰+ è©•ä¼°ï¼ˆevaluateï¼‰+ éƒ¨ç½²ï¼ˆgradio/hubï¼‰
> ç„¡ç¸«æ•´åˆï¼Œå½¢æˆå®Œæ•´çš„ AI é–‹ç™¼æµç¨‹ï¼š
>
> ã€ŒTrain â†’ Fine-Tune â†’ Evaluate â†’ Deploy â†’ Shareã€
> å¸¸è¦‹æ‡‰ç”¨åŒ…æ‹¬ï¼š
> ğŸ§  NLPï¼šæƒ…æ„Ÿåˆ†æã€ç¿»è­¯ã€æ‘˜è¦ã€å•ç­”
> ğŸ–¼ï¸ CVï¼šå½±åƒåˆ†é¡ã€ç‰©ä»¶åµæ¸¬
> ğŸµ Audioï¼šèªéŸ³è¾¨è­˜ã€éŸ³æ¨‚ç”Ÿæˆ
> ğŸ’¬ Chatbotï¼šRAGã€LLM æ‡‰ç”¨é–‹ç™¼
> ğŸš€ å¾®èª¿ï¼šLoRAã€PEFTã€QLoRA

# ğŸ¤— Hugging Face å„é …åŠŸèƒ½èˆ‡å¸¸ç”¨å‡½æ•¸ç¸½è¦½è¡¨  
> **ä¸­è‹±å°ç…§ + æ¨¡çµ„åˆ†é¡ + ç¯„ä¾‹èªªæ˜**

---

## ä¸€ã€ä¸»è¦æ¨¡çµ„ï¼ˆCore Modules Overviewï¼‰

| æ¨¡çµ„åç¨± | ä¸­æ–‡èªªæ˜ | åŠŸèƒ½é‡é» |
|------------|------------|------------|
| `transformers` | æ¨¡å‹èˆ‡æ¨è«– | é è¨“ç·´æ¨¡å‹è¼‰å…¥ã€å¾®èª¿ã€æ¨è«– |
| `datasets` | è³‡æ–™æ¨¡çµ„ | æä¾› 10,000+ NLP / CV / Audio è³‡æ–™é›† |
| `tokenizers` | åˆ†è©æ¨¡çµ„ | å¿«é€Ÿæ–·è©èˆ‡å­è©ç·¨ç¢¼ï¼ˆBPEã€WordPieceï¼‰ |
| `evaluate` | è©•ä¼°æ¨¡çµ„ | å„é¡æŒ‡æ¨™ï¼ˆBLEUã€ROUGEã€F1ã€Accuracyï¼‰ |
| `accelerate` | åŠ é€Ÿæ¨¡çµ„ | åˆ†æ•£å¼è¨“ç·´èˆ‡å¤š GPU æ”¯æ´ |
| `peft` | åƒæ•¸é«˜æ•ˆå¾®èª¿ | LoRAã€Prefix Tuningã€Adapters |
| `diffusers` | ç”Ÿæˆæ¨¡å‹ï¼ˆå½±åƒï¼‰ | Stable Diffusionã€ControlNet ç­‰ |
| `gradio` | Web ä»‹é¢ | å¿«é€Ÿå»ºç«‹æ¨¡å‹ Demo UI |
| `huggingface_hub` | æ¨¡å‹ä¸­å¿ƒ API | ä¸Šå‚³ / ä¸‹è¼‰æ¨¡å‹ã€è³‡æ–™é›†ã€ç©ºé–“ |
| `trl` | å¼·åŒ–å­¸ç¿’å¾®èª¿ | æ”¯æ´ RLHF / PPO / DPO ç­‰ |
| `optimum` | éƒ¨ç½²èˆ‡æœ€ä½³åŒ– | ONNX / TensorRT / Intel / Apple å„ªåŒ–éƒ¨ç½² |

---

## äºŒã€Transformers æ¨¡çµ„ï¼ˆ`transformers`ï¼‰

| å‡½æ•¸ / é¡åˆ¥ | åŠŸèƒ½èªªæ˜ | ç¯„ä¾‹ | å‚™è¨» |
|---------------|------------|-------|------|
| `pipeline()` | å¿«é€Ÿæ¨è«–ç®¡ç·š | `pipeline("sentiment-analysis")` | è‡ªå‹•è¼‰å…¥æ¨¡å‹ |
| `AutoTokenizer.from_pretrained()` | è¼‰å…¥å°æ‡‰ tokenizer | `AutoTokenizer.from_pretrained("bert-base-uncased")` | è‡ªå‹•åŒ¹é…æ¨¡å‹ |
| `AutoModel.from_pretrained()` | è¼‰å…¥æ¨¡å‹ | `AutoModel.from_pretrained("bert-base-uncased")` | â€” |
| `AutoModelForSequenceClassification` | åºåˆ—åˆ†é¡æ¨¡å‹ | â€” | ç”¨æ–¼æƒ…æ„Ÿåˆ†æã€ä¸»é¡Œåˆ†é¡ |
| `AutoModelForCausalLM` | èªè¨€ç”Ÿæˆæ¨¡å‹ | GPT ç³»åˆ— | â€” |
| `AutoModelForTokenClassification` | å‘½åå¯¦é«”è¾¨è­˜æ¨¡å‹ | â€” | NER ä»»å‹™ |
| `AutoModelForQuestionAnswering` | å•ç­”æ¨¡å‹ | â€” | SQuAD ä»»å‹™ |
| `AutoModelForImageClassification` | åœ–åƒåˆ†é¡æ¨¡å‹ | â€” | ViT / DeiT æ¨¡å‹ |
| `.generate()` | æ–‡å­—ç”Ÿæˆ | `model.generate(inputs, max_new_tokens=50)` | æ”¯æ´ beam search |
| `.from_pretrained()` | è¼‰å…¥ Hugging Face Hub æ¨¡å‹ | æ”¯æ´æœ¬åœ°æˆ–é ç«¯ | â€” |
| `.save_pretrained()` | å„²å­˜æ¨¡å‹èˆ‡ tokenizer | `model.save_pretrained("./model")` | â€” |
| `.to("cuda")` | GPU åŠ é€Ÿ | â€” | CUDA / MPS / CPU |

---

## ä¸‰ã€Tokenizers æ¨¡çµ„ï¼ˆ`tokenizers`ï¼‰

| å‡½æ•¸ / é¡åˆ¥ | åŠŸèƒ½èªªæ˜ | ç¯„ä¾‹ | å‚™è¨» |
|---------------|------------|-------|------|
| `Tokenizer()` | å»ºç«‹è‡ªè¨‚åˆ†è©å™¨ | `Tokenizer(models.BPE())` | â€” |
| `BPE`, `WordPiece`, `Unigram` | åˆ†è©æ¼”ç®—æ³• | â€” | å¯è¨“ç·´è‡ªè¨‚èªæ–™ |
| `.train_from_iterator()` | è‡ªå»ºå­—å…¸ | `tokenizer.train_from_iterator(texts)` | â€” |
| `.encode()` / `.decode()` | å­—ä¸²è½‰ ID / åè§£ | â€” | â€” |
| `.save()` / `.from_file()` | å„²å­˜èˆ‡è¼‰å…¥åˆ†è©å™¨ | â€” | â€” |
| `.get_vocab()` | å–å¾—è©å½™è¡¨ | â€” | â€” |

---

## å››ã€Datasets æ¨¡çµ„ï¼ˆ`datasets`ï¼‰

| å‡½æ•¸ / é¡åˆ¥ | åŠŸèƒ½èªªæ˜ | ç¯„ä¾‹ | å‚™è¨» |
|---------------|------------|-------|------|
| `load_dataset()` | è¼‰å…¥è³‡æ–™é›† | `load_dataset("imdb")` | è‡ªå‹•ä¸‹è¼‰ |
| `load_from_disk()` | è¼‰å…¥æœ¬åœ°è³‡æ–™é›† | â€” | â€” |
| `.map()` | æ˜ å°„å‡½æ•¸è½‰æ›è³‡æ–™ | `dataset.map(tokenize_function)` | ç”¨æ–¼åˆ†è© |
| `.filter()` | ç¯©é¸æ¨£æœ¬ | `dataset.filter(lambda x: len(x["text"])>50)` | â€” |
| `.shuffle()` | æ‰“äº‚è³‡æ–™ | â€” | â€” |
| `.train_test_split()` | è³‡æ–™åˆ†å‰² | â€” | â€” |
| `.to_pandas()` | è½‰æˆ DataFrame | â€” | â€” |
| `.save_to_disk()` | å„²å­˜è™•ç†å¾Œè³‡æ–™ | â€” | â€” |

---

## äº”ã€Evaluate æ¨¡çµ„ï¼ˆ`evaluate`ï¼‰

| å‡½æ•¸åç¨± | åŠŸèƒ½èªªæ˜ | ç¯„ä¾‹ | å‚™è¨» |
|------------|------------|-------|------|
| `load("accuracy")` | æº–ç¢ºç‡æŒ‡æ¨™ | `metric = evaluate.load("accuracy")` | â€” |
| `load("f1")` | F1-score | â€” | â€” |
| `load("precision")`, `load("recall")` | ç²¾ç¢ºç‡ / å¬å›ç‡ | â€” | â€” |
| `load("bleu")` | BLEU åˆ†æ•¸ï¼ˆç¿»è­¯ï¼‰ | â€” | â€” |
| `load("rouge")` | ROUGE åˆ†æ•¸ï¼ˆæ‘˜è¦ï¼‰ | â€” | â€” |
| `.compute(predictions, references)` | è¨ˆç®—æŒ‡æ¨™ | â€” | è¼¸å…¥é æ¸¬èˆ‡çœŸå¯¦å€¼ |

---

## å…­ã€PEFT æ¨¡çµ„ï¼ˆParameter-Efficient Fine-Tuningï¼‰

| æ–¹æ³• / æ¨¡å‹ | åŠŸèƒ½èªªæ˜ | å¸¸ç”¨åƒæ•¸ | å‚™è¨» |
|---------------|------------|-------------|------|
| `LoraConfig()` | LoRA å¾®èª¿è¨­å®š | `r`, `alpha`, `target_modules` | â€” |
| `get_peft_model()` | å°‡æ¨¡å‹è½‰ç‚º PEFT æ¨¡å‹ | `get_peft_model(base_model, peft_config)` | â€” |
| `.save_pretrained()` | å„²å­˜ LoRA æ¬Šé‡ | â€” | å¯åˆä½µè‡³ä¸»æ¨¡å‹ |
| `.merge_and_unload()` | åˆä½µå¾®èª¿æ¬Šé‡ | â€” | â€” |

---

## ä¸ƒã€Diffusers æ¨¡çµ„ï¼ˆå½±åƒç”Ÿæˆï¼‰

| é¡åˆ¥ / å‡½æ•¸ | åŠŸèƒ½èªªæ˜ | ç¯„ä¾‹ | å‚™è¨» |
|---------------|------------|-------|------|
| `StableDiffusionPipeline()` | è¼‰å…¥ Stable Diffusion æ¨¡å‹ | `pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")` | â€” |
| `.to("cuda")` | GPU åŠ é€Ÿ | â€” | â€” |
| `.enable_attention_slicing()` | é™ä½ VRAM ä½¿ç”¨ | â€” | â€” |
| `.__call__()` | æ–‡ç”Ÿåœ– | `pipe("A cat wearing sunglasses")` | â€” |
| `.save_pretrained()` | å„²å­˜æ¨¡å‹ | â€” | â€” |

---

## å…«ã€Accelerate æ¨¡çµ„ï¼ˆå¤š GPU / åˆ†æ•£å¼è¨“ç·´ï¼‰

| å‡½æ•¸ / é¡åˆ¥ | åŠŸèƒ½èªªæ˜ | ç¯„ä¾‹ |
|---------------|------------|-------|
| `Accelerator()` | å»ºç«‹åŠ é€Ÿå™¨ | `accelerator = Accelerator()` |
| `.prepare()` | åŒ…è£æ¨¡å‹ / è³‡æ–™ | `model, optimizer, dataloader = accelerator.prepare(...)` |
| `.unwrap_model()` | æ‹†è§£åŒ…è£æ¨¡å‹ | â€” |
| `.print()` | åˆ†æ•£å¼æ‰“å° | â€” |

---

## ä¹ã€Hugging Face Hubï¼ˆé›²ç«¯æ¨¡å‹ä¸­å¿ƒï¼‰

| å‡½æ•¸ / é¡åˆ¥ | åŠŸèƒ½èªªæ˜ | ç¯„ä¾‹ |
|---------------|------------|-------|
| `huggingface_hub.login()` | ç™»å…¥å¸³è™Ÿ | `huggingface_hub.login(token="hf_...")` |
| `hf_hub_download()` | ä¸‹è¼‰æ¨¡å‹æˆ–è³‡æ–™ | `hf_hub_download(repo_id="bert-base-uncased")` |
| `HfApi()` | æ“ä½œ Hub API | `api = HfApi()` |
| `api.upload_file()` | ä¸Šå‚³æª”æ¡ˆ | â€” |
| `api.list_models()` / `api.list_datasets()` | æŸ¥è©¢æ¨¡å‹ / è³‡æ–™ | â€” |
| `model_card` | æ¨¡å‹èªªæ˜æ–‡ä»¶ | Markdown æ ¼å¼ | â€” |

---

## åã€Gradio æ¨¡çµ„ï¼ˆWeb Demo ä»‹é¢ï¼‰

| å‡½æ•¸ / é¡åˆ¥ | åŠŸèƒ½èªªæ˜ | ç¯„ä¾‹ |
|---------------|------------|-------|
| `gr.Interface()` | å»ºç«‹äº’å‹•ä»‹é¢ | `gr.Interface(fn=predict, inputs="text", outputs="label")` |
| `.launch()` | å•Ÿå‹•ä¼ºæœå™¨ | â€” |
| `Blocks()` | è¤‡åˆå¼ä»‹é¢ | `with gr.Blocks() as demo:` | æ”¯æ´å¤šæ¨¡çµ„ UI |
| `Textbox`, `Image`, `Audio`, `Dropdown` | äº’å‹•å…ƒä»¶ | â€” |
| `.queue()` | éåŒæ­¥åŸ·è¡Œ | â€” |

---

## åä¸€ã€Transformers - æ¨è«–èˆ‡å¾®èª¿ç¯„ä¾‹ï¼ˆNLPï¼‰

| ä»»å‹™ | ä½¿ç”¨æ¨¡å‹ | ç¯„ä¾‹ç¨‹å¼ |
|-------|------------|-------------|
| **æƒ…æ„Ÿåˆ†æ** | `distilbert-base-uncased-finetuned-sst-2-english` | `pipeline("sentiment-analysis")("I love AI!")` |
| **ç¿»è­¯** | `Helsinki-NLP/opus-mt-en-fr` | `pipeline("translation_en_to_fr")("Hello world")` |
| **æ‘˜è¦** | `facebook/bart-large-cnn` | `pipeline("summarization")(text)` |
| **å•ç­”ç³»çµ±** | `deepset/roberta-base-squad2` | `pipeline("question-answering")(...)` |
| **æ–‡æœ¬ç”Ÿæˆ** | `gpt2`, `tiiuae/falcon-7b-instruct` | `pipeline("text-generation")("Once upon a time...")` |
| **é›¶æ¨£æœ¬åˆ†é¡** | `facebook/bart-large-mnli` | `pipeline("zero-shot-classification")("This is amazing", ["positive", "negative"])` |

---

## åäºŒã€ç¯„ä¾‹ï¼šç°¡æ˜“ç”Ÿæˆæ¨¡å‹ API

```python
from fastapi import FastAPI
from transformers import pipeline

app = FastAPI()
generator = pipeline("text-generation", model="gpt2")

@app.get("/generate")
def generate(prompt: str):
    result = generator(prompt, max_new_tokens=50)
    return {"result": result[0]["generated_text"]}
```

## åä¸‰ã€ç¯„ä¾‹ï¼šLoRA å¾®èª¿ + æ¨è«–
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model

model_name = "meta-llama/Llama-2-7b-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

lora_config = LoraConfig(r=8, lora_alpha=32, target_modules=["q_proj", "v_proj"])
model = get_peft_model(model, lora_config)

prompt = "Explain quantum computing in simple terms"
inputs = tokenizer(prompt, return_tensors="pt")
print(model.generate(**inputs, max_new_tokens=60))

```
