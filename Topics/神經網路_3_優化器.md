# Optimization
- Optimization Problem  最佳化問題/優化問題
- optimizer(優化器) == 解決Optimization Problem的演算法或方法 

# 神經網路_優化器1
- 梯度下降法Gradient Descent (GD)
  - Gradient Descent (GD)
  - Batch Gradient Descent(BGD)
  - Mini-Batch Gradient Descent (MBGD)
  - Stochastic Gradient Descent (SGD)
  - 推薦影片:https://www.youtube.com/watch?v=yKKNr-QKz2Q
  - 梯度消失問題(Gradient Vanishing)與梯度爆炸問題(Gradient Exploding)
- Momentum
- Nesterov accelerated gradient (NAG1983) 
## 神經網路_優化器2:可動態調整的學習率(Adaptive Learning Rates)
- 第一次學習==> 比較慢 | 第二次學習==> 稍微快    | 更多學習==> 學習更快
- 所以 學習率不應該是固定的 .......
- Adagrad
- RMSprop
  - 看看大師的演講 https://www.youtube.com/watch?v=defQQqkXEfE
  - https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf
- Adam(2015)
  - Momentum是「計算參數更新方向前會考慮前一次參數更新的方向」
  - RMSprop則是「在學習率上依據梯度的大小對學習率進行加強或是衰減」。
  - Adam則是兩者合併加強版本(Momentum+RMSprop+各自做偏差的修正)。
  - Kingma, D. P., & Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, 1–13
- Nadam
  - http://cs229.stanford.edu/proj2015/054_report.pdf 




## 推薦影片
- 推薦影片教學:史丹佛大學著名的CS231n課程
  - Lecture 7 | Training Neural Networks II
  - https://www.youtube.com/watch?v=_JB0AO7QxSA&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=7
- https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a
- https://imgur.com/a/Hqolp#NKsFHJb

## 好書推薦
- Deep Learning：用Python進行深度學習的基礎理論實作
  - 作者： 斎藤康毅  譯者： 吳嘉芳 出版社：歐萊禮  出版日期：2017/08/17
  - https://github.com/oreilly-japan/deep-learning-from-scratch
  - 第五章 誤差反向傳播法
  - 第六章 與學習有關的技巧 
  - 看看 common目錄底下的程式 ==> 學習如何撰寫Python套件(package)





