# ç¥ç¶“ç¶²è·¯
- å–®ç´”æ„ŸçŸ¥æ©Ÿæ¨¡å‹(æ²’æœ‰å­¸ç¿’åŠŸèƒ½)
  - ä½¿ç”¨æ„ŸçŸ¥æ©Ÿæ¨¡å‹å»ºæ§‹ç°¡å–®é‚è¼¯é–˜é‹ä½œ
  - ä½¿ç”¨å¤šå±¤æ„ŸçŸ¥æ©Ÿå»ºæ§‹NORé‚è¼¯é–˜é‹ä½œ
- ç›£ç£å¼(æ©Ÿå™¨å­¸ç¿’) ==> çœ‹çœ‹æ©Ÿå™¨æ˜¯å¦‚ä½•`å­¸ç¿’`
  - ä½¿ç”¨å¤šå±¤æ„ŸçŸ¥æ©Ÿé€²è¡Œåˆ†é¡
  - ä½¿ç”¨å¤šå±¤æ„ŸçŸ¥æ©Ÿé€²è¡Œå›æ­¸

# å–®ç´”æ„ŸçŸ¥æ©Ÿæ¨¡å‹(æ²’æœ‰å­¸ç¿’åŠŸèƒ½)
## ä½¿ç”¨æ„ŸçŸ¥æ©Ÿæ¨¡å‹å»ºæ§‹ç°¡å–®é‚è¼¯é–˜é‹ä½œ
- AND é‚è¼¯é–˜çš„æ„ŸçŸ¥æ©Ÿæ¨¡å‹
```python
import numpy as np


def AND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.6, 0.4])
    b = -0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1

if __name__ == '__main__':
    for xs in [(0, 0), (1, 0), (0, 1), (1, 1)]:
        y = AND(xs[0], xs[1])
        print(str(xs) + " -> " + str(y))
```
- NAND é‚è¼¯é–˜çš„æ„ŸçŸ¥æ©Ÿæ¨¡å‹
```python
import numpy as np


def NAND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([-0.6, -0.6])
    b = 0.8
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1

if __name__ == '__main__':
    for xs in [(0, 0), (1, 0), (0, 1), (1, 1)]:
        y = NAND(xs[0], xs[1])
        print(str(xs) + " -> " + str(y))
```
- OR é‚è¼¯é–˜çš„æ„ŸçŸ¥æ©Ÿæ¨¡å‹
```python
import numpy as np


def OR(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.4
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1

if __name__ == '__main__':
    for xs in [(0, 0), (1, 0), (0, 1), (1, 1)]:
        y = OR(xs[0], xs[1])
        print(str(xs) + " -> " + str(y))
```
- NOR é‚è¼¯é–˜çš„æ„ŸçŸ¥æ©Ÿæ¨¡å‹
```python
import numpy as np


def NOR(x1, x2):
    x = np.array([x1, x2])
    w = np.array([-0.5, -0.5])
    b = 0.4
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1

if __name__ == '__main__':
    for xs in [(0, 0), (1, 0), (0, 1), (1, 1)]:
        y = NOR(xs[0], xs[1])
        print(str(xs) + " -> " + str(y))
```
## ä½¿ç”¨å¤šå±¤æ„ŸçŸ¥æ©Ÿ(MLP)å»ºæ§‹NORé‚è¼¯é–˜é‹ä½œ
```python

def XOR(x1, x2):
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    y = AND(s1, s2)
    return y

if __name__ == '__main__':
    for xs in [(0, 0), (1, 0), (0, 1), (1, 1)]:
        y = XOR(xs[0], xs[1])
        print(str(xs) + " -> " + str(y))
```
# ç›£ç£å¼(æ©Ÿå™¨å­¸ç¿’) ==> åˆ†é¡--è¿´æ­¸åˆ†æ(Regression Analysisï¼‰
#### ä½¿ç”¨æ„ŸçŸ¥æ©Ÿé€²è¡Œåˆ†é¡ 
- ä½¿ç”¨ç‰©ä»¶å°å‘æŠ€è¡“æ’°å¯«æ„ŸçŸ¥æ©Ÿé¡åˆ¥class Perceptron
```python
import numpy as np

class Perceptron:
    """
    ç°¡å–®æ„ŸçŸ¥æ©Ÿ (Perceptron) æ¨¡å‹å¯¦ä½œ
    """

    def __init__(self, learning_rate=0.1, n_iters=100):
        self.lr = learning_rate
        self.n_iters = n_iters
        self.activation_func = self._unit_step
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        """
        è¨“ç·´ æ„ŸçŸ¥æ©Ÿæ¨¡å‹
        X: (æ¨£æœ¬æ•¸, ç‰¹å¾µæ•¸)
        y: æ¨™ç±¤(0 æˆ– 1)
        """
        n_samples, n_features = X.shape
        # åˆå§‹åŒ–æ¬Šé‡èˆ‡åç½® ==> å…¨éƒ¨è¨­ç‚º0
        self.weights = np.zeros(n_features)
        self.bias = 0

        # å°‡æ¨™ç±¤è½‰ç‚º -1 / 1ï¼ˆæ„ŸçŸ¥æ©Ÿæ›´æ–°è¦å‰‡ï¼‰
        y_ = np.where(y <= 0, -1, 1)

        for _ in range(self.n_iters):
            for idx, x_i in enumerate(X):
                # é æ¸¬
                linear_output = np.dot(x_i, self.weights) + self.bias
                y_pred = self.activation_func(linear_output)
                # æ›´æ–°è¦å‰‡ï¼šw â† w + lr * (y_true - y_pred) * x
                update = self.lr * (y_[idx] - y_pred)
                self.weights += update * x_i
                self.bias += update

    def predict(self, X):
        """
        ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹é€²è¡Œ  é æ¸¬
        """
        linear_output = np.dot(X, self.weights) + self.bias
        y_pred = self.activation_func(linear_output)
        return np.where(y_pred > 0, 1, 0)

    def _unit_step(self, x):
        """
        æ¿€æ´»å‡½æ•¸ Activation function ==> ä½¿ç”¨éšæ¢¯å‡½æ•¸Step Function
        """
        return np.where(x >= 0, 1, -1)

# === æ¸¬è©¦è³‡æ–™ï¼šé‚è¼¯ AND é‹ç®— ===
X = np.array([[0,0], [0,1], [1,0], [1,1]])
y = np.array([0, 0, 0, 1])  # AND é‹ç®—çµæœ

# å»ºç«‹ä¸¦è¨“ç·´æ„ŸçŸ¥æ©Ÿæ¨¡å‹
clf = Perceptron(learning_rate=0.1, n_iters=10)
clf.fit(X, y)

# é æ¸¬èˆ‡çµæœ
predictions = clf.predict(X)
print("é æ¸¬çµæœ:", predictions)
print("çœŸå¯¦æ¨™ç±¤:", y)
print("æ¬Šé‡:", clf.weights)
print("åç½®:", clf.bias)
```
```
é æ¸¬çµæœ: [0 0 0 1]
çœŸå¯¦æ¨™ç±¤: [0 0 0 1]
æ¬Šé‡: [0.4 0.2]
åç½®: -0.4000000000000001
```
#### ä½¿ç”¨å¤šå±¤æ„ŸçŸ¥æ©Ÿé€²è¡ŒXORåˆ†é¡
```python
## è¼‰å…¥å¥—ä»¶
#from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
#from sklearn import svm

## è¼¸å…¥è³‡æ–™
y = [0, 1, 1, 0]
X = [[0, 0], [0, 1], [1, 0], [1, 1]]

## å»ºç«‹ åˆ†é¡å™¨clf 
clf = MLPClassifier(solver='lbfgs', activation='logistic', hidden_layer_sizes=(2,), max_iter=100, random_state=20)

## è¨“ç·´ åˆ†é¡å™¨ ==> ä½¿ç”¨fit()å‡½æ•¸
clf.fit(X, y)

## é æ¸¬ ==> ä½¿ç”¨predict()å‡½æ•¸
predictions = clf.predict(X)

## è¨ˆç®—è©•ä¼°æŒ‡æ¨™ ==> ä½¿ç”¨score()å‡½æ•¸
print('Accuracy: %s' % clf.score(X, y))

for i, p in enumerate(predictions[:10]):
    print('True: %s, Predicted: %s' % (y[i], p))
```
#### ä½¿ç”¨æ„ŸçŸ¥æ©Ÿé€²è¡Œè¿´æ­¸åˆ†æ(Regression Analysisï¼‰
- CHATGPT:ç°¡å–®å»ºç«‹ç·šæ€§å›æ­¸çš„è³‡æ–™ å†ä½¿ç”¨pythonç‰©ä»¶å°å‘æ–¹å¼æ’°å¯«æ„ŸçŸ¥æ©Ÿ ä¸¦åˆ†æè³‡æ–™
  - å»ºç«‹ç°¡å–®çš„ç·šæ€§è³‡æ–™é›†ï¼ˆä¾‹å¦‚ ğ‘¦=3ğ‘¥+5+å™ªéŸ³)
  - ä»¥ç‰©ä»¶å°å‘ï¼ˆOOPï¼‰æ–¹å¼å¯¦ä½œæ„ŸçŸ¥æ©Ÿï¼ˆPerceptronï¼‰æ¨¡å‹é€²è¡Œè¿´æ­¸åˆ†æ
  - åˆ†æè¨“ç·´çµæœèˆ‡å¯è¦–åŒ–é æ¸¬
```python
## ä¸€ã€å»ºç«‹ç·šæ€§è³‡æ–™
import numpy as np
import matplotlib.pyplot as plt

# ç”¢ç”Ÿç·šæ€§è³‡æ–™
np.random.seed(42)
X = np.linspace(0, 10, 50).reshape(-1, 1)
y = 3 * X + 5 + np.random.randn(50, 1) * 2  # y = 3x + 5 + å™ªéŸ³

plt.scatter(X, y, color='blue', label='Data')
plt.title("Linear Data Example")
plt.xlabel("X")
plt.ylabel("y")
plt.legend()
plt.show()

## äºŒã€ä»¥ç‰©ä»¶å°å‘æ–¹å¼å¯¦ä½œæ„ŸçŸ¥æ©Ÿ (Perceptron Regression)

class PerceptronRegressor:
    """
    ä¸€å€‹ç°¡å–®çš„æ„ŸçŸ¥æ©Ÿè¿´æ­¸æ¨¡å‹ (é¡ä¼¼ç·šæ€§è¿´æ­¸)
    y_pred = w * x + b
    ä½¿ç”¨æ¢¯åº¦ä¸‹é™ (Gradient Descent) æ›´æ–°æ¬Šé‡
    """
    def __init__(self, lr=0.01, epochs=1000):
        self.lr = lr
        self.epochs = epochs
        self.w = None
        self.b = None
        self.loss_history = []

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.w = np.zeros((n_features, 1))
        self.b = 0

        for epoch in range(self.epochs):
            y_pred = np.dot(X, self.w) + self.b
            error = y_pred - y
            loss = np.mean(error ** 2)

            # è¨˜éŒ„æå¤±
            self.loss_history.append(loss)

            # æ›´æ–°åƒæ•¸
            dw = (2 / n_samples) * np.dot(X.T, error)
            db = (2 / n_samples) * np.sum(error)
            self.w -= self.lr * dw
            self.b -= self.lr * db

    def predict(self, X):
        return np.dot(X, self.w) + self.b

## ä¸‰ã€è¨“ç·´èˆ‡åˆ†æ

# åˆå§‹åŒ–ä¸¦è¨“ç·´æ¨¡å‹
model = PerceptronRegressor(lr=0.01, epochs=1000)
model.fit(X, y)

# é æ¸¬
y_pred = model.predict(X)

# é¡¯ç¤ºè¨“ç·´çµæœ
print(f"æ¬Šé‡ w = {model.w.flatten()[0]:.3f}, åå·® b = {model.b:.3f}")

# å¯è¦–åŒ–çµæœ
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.scatter(X, y, label='Data')
plt.plot(X, y_pred, color='red', label='Model Prediction')
plt.title("Perceptron Regression Result")
plt.xlabel("X")
plt.ylabel("y")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(model.loss_history)
plt.title("Loss Curve")
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.show()
```
## å¤šå±¤æ„ŸçŸ¥æ©Ÿçš„æ¬Šé‡æ›´æ–°æ©Ÿåˆ¶ ==> åå‘å‚³æ’­(backpropagation)
