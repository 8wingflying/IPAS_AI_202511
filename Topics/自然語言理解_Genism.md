# ğŸ“š **Gensim** 
> **Gensim** æ˜¯ NLP ä¸­æœ€å¼·å¤§çš„ **ä¸»é¡Œå»ºæ¨¡èˆ‡èªæ„åˆ†æå·¥å…·ä¹‹ä¸€**ï¼Œ  
> æ”¯æ´ï¼š
> - ğŸ“– æ–‡æœ¬å‰è™•ç†ï¼ˆTokenization / Stopwords / Lemmatizationï¼‰  
> - ğŸ§® èªæ–™è½‰æ›ï¼ˆBOW / TF-IDF / LSI / LDA / NMFï¼‰  
> - ğŸ§  è©å‘é‡æ¨¡å‹ï¼ˆWord2Vec / FastText / Doc2Vecï¼‰  
> - ğŸ” æ–‡ä»¶ç›¸ä¼¼åº¦èˆ‡èªæ„æœå°‹  
> - âš™ï¸ é«˜æ•ˆèƒ½èˆ‡è¨˜æ†¶é«”å„ªåŒ–ï¼ˆæ”¯æ´å¤§å‹èªæ–™æµå¼è¨“ç·´ï¼‰  

---


# ğŸ§  Gensim å„é …åŠŸèƒ½èˆ‡å¸¸ç”¨å‡½æ•¸ç¸½è¦½è¡¨  

---

## ä¸€ã€Gensim æ¦‚è¦ï¼ˆOverviewï¼‰

| æ¨¡çµ„åç¨± | ä¸­æ–‡èªªæ˜ | åŠŸèƒ½é‡é» |
|------------|------------|------------|
| `gensim.corpora` | èªæ–™èˆ‡å­—å…¸æ¨¡çµ„ | å»ºç«‹è©å…¸ã€èªæ–™åº«ã€Bag-of-Words æ¨¡å‹ |
| `gensim.models` | æ¨¡å‹æ¨¡çµ„ | Word2Vecã€Doc2Vecã€LDAã€TF-IDFã€LSI ç­‰ |
| `gensim.similarities` | ç›¸ä¼¼åº¦æ¨¡çµ„ | æ–‡ä»¶ç›¸ä¼¼åº¦è¨ˆç®—èˆ‡æª¢ç´¢ |
| `gensim.parsing` | æ–‡æœ¬å‰è™•ç† | åˆ†è©ã€å»é™¤åœç”¨è©ã€è©å½¢é‚„åŸ |
| `gensim.utils` | å·¥å…·æ¨¡çµ„ | è³‡æ–™è¼‰å…¥ã€tokenizeã€å„²å­˜æ¨¡å‹ |
| `gensim.downloader` | æ¨¡å‹ä¸‹è¼‰ | é è¨“ç·´è©å‘é‡ï¼ˆå¦‚ GloVeã€word2vec-google-newsï¼‰ |

---

## äºŒã€æ–‡æœ¬è™•ç†èˆ‡èªæ–™å»ºç«‹ï¼ˆText Preprocessingï¼‰

| å‡½æ•¸ / é¡åˆ¥ | åŠŸèƒ½èªªæ˜ | ç¯„ä¾‹ | å‚™è¨» |
|--------------|------------|-------|------|
| `simple_preprocess()` | åˆ†è© + å°å¯« + å»é™¤ç¬¦è™Ÿ | `gensim.utils.simple_preprocess("Hello World!")` | è¼¸å‡ºè©åˆ—è¡¨ |
| `remove_stopwords()` | ç§»é™¤åœç”¨è© | `from gensim.parsing.preprocessing import remove_stopwords` | é è¨­è‹±æ–‡ |
| `strip_punctuation()` | å»é™¤æ¨™é»ç¬¦è™Ÿ | â€” | â€” |
| `strip_numeric()` | å»é™¤æ•¸å­— | â€” | â€” |
| `strip_short()` | ç§»é™¤çŸ­è©ï¼ˆé•·åº¦ â‰¤ Nï¼‰ | `strip_short(text, minsize=3)` | â€” |
| `preprocess_string()` | ä¸€æ¬¡å®Œæˆæ¸…ç†æµç¨‹ | `from gensim.parsing.preprocessing import preprocess_string` | çµåˆå¤šå€‹ filter |
| `split_sentences()` | å¥å­åˆ‡å‰² | `split_sentences(text)` | â€” |

---

## ä¸‰ã€å­—å…¸èˆ‡èªæ–™åº«ï¼ˆDictionary & Corpusï¼‰

| é¡åˆ¥ / å‡½æ•¸ | åŠŸèƒ½èªªæ˜ | ç¯„ä¾‹ | å‚™è¨» |
|---------------|------------|-------|------|
| `Dictionary()` | å»ºç«‹è©å…¸ï¼ˆå­—â†’IDï¼‰ | `id2word = corpora.Dictionary(texts)` | â€” |
| `.token2id` | è©èˆ‡ç´¢å¼•æ˜ å°„ | `id2word.token2id` | â€” |
| `.doc2bow()` | å°‡æ–‡ä»¶è½‰ç‚º Bag-of-Words | `id2word.doc2bow(["apple","banana"])` | è¼¸å‡º `(id, count)` |
| `.filter_extremes()` | éæ¿¾é«˜é » / ä½é »è© | `no_below`, `no_above` | â€” |
| `.save()` / `.load()` | å„²å­˜ / è¼‰å…¥è©å…¸ | â€” | â€” |
| `MmCorpus()` / `BleiCorpus()` / `TextCorpus()` | ä¸åŒèªæ–™æ ¼å¼ | ç”¨æ–¼å¤§å‹èªæ–™è¨“ç·´ | â€” |

---

## å››ã€TF-IDF æ¨¡å‹ï¼ˆTerm Frequencyâ€“Inverse Document Frequencyï¼‰

| é¡åˆ¥ / å‡½æ•¸ | åŠŸèƒ½èªªæ˜ | ç¯„ä¾‹ | å‚™è¨» |
|---------------|------------|-------|------|
| `TfidfModel()` | å»ºç«‹ TF-IDF æ¨¡å‹ | `tfidf = models.TfidfModel(corpus)` | è¨“ç·´æ¨¡å‹ |
| `.transform()` | å°‡èªæ–™è½‰ç‚º TF-IDF | `tfidf[doc_bow]` | â€” |
| `.save()` / `.load()` | æ¨¡å‹å„²å­˜ / è¼‰å…¥ | â€” | â€” |

---

## äº”ã€ä¸»é¡Œæ¨¡å‹ï¼ˆTopic Modelingï¼‰

| æ¨¡å‹åç¨± | åŠŸèƒ½èªªæ˜ | å¸¸ç”¨åƒæ•¸ | ç¯„ä¾‹ |
|------------|------------|-------------|------|
| `LdaModel()` | Latent Dirichlet Allocation ä¸»é¡Œæ¨¡å‹ | `num_topics`, `passes`, `alpha`, `eta` | `lda = LdaModel(corpus, id2word, num_topics=5)` |
| `LdaMulticore()` | å¤šæ ¸å¿ƒ LDA | `workers` | æé«˜é‹ç®—æ•ˆç‡ |
| `LsiModel()` | Latent Semantic Indexingï¼ˆæ½›åœ¨èªæ„ç´¢å¼•ï¼‰ | `num_topics` | `lsi = LsiModel(tfidf_corpus, id2word)` |
| `HdpModel()` | Hierarchical Dirichlet Process | è‡ªå‹•æ±ºå®šä¸»é¡Œæ•¸ | `hdp = HdpModel(corpus, id2word)` |
| `Nmf()` | éè² çŸ©é™£åˆ†è§£ä¸»é¡Œæ¨¡å‹ | `num_topics`, `passes` | `nmf = Nmf(corpus, id2word)` |
| `.show_topics()` | é¡¯ç¤ºä¸»é¡Œå­—è© | `num_words=10` | â€” |
| `.get_document_topics()` | æ–‡ä»¶ä¸»é¡Œåˆ†ä½ˆ | â€” | â€” |

---

## å…­ã€è©å‘é‡æ¨¡å‹ï¼ˆWord Embedding Modelsï¼‰

| æ¨¡å‹åç¨± | åŠŸèƒ½èªªæ˜ | å¸¸ç”¨åƒæ•¸ | ç¯„ä¾‹ |
|------------|------------|-------------|------|
| `Word2Vec()` | è©å‘é‡æ¨¡å‹ | `vector_size`, `window`, `min_count`, `sg`, `epochs` | `w2v = Word2Vec(sentences, vector_size=100)` |
| `FastText()` | å­—æ ¹è©å‘é‡æ¨¡å‹ | `min_n`, `max_n` | `ft = FastText(sentences, vector_size=100)` |
| `Doc2Vec()` | æ–‡ä»¶å‘é‡æ¨¡å‹ | `dm`, `vector_size`, `window` | `d2v = Doc2Vec(docs, vector_size=100)` |
| `.train()` | æ¨¡å‹è¨“ç·´ | `total_examples`, `epochs` | â€” |
| `.wv.most_similar()` | ç›¸ä¼¼è©æŸ¥è©¢ | `positive`, `topn` | `w2v.wv.most_similar("king")` |
| `.wv.similarity()` | è©è·é›¢ | `w2v.wv.similarity("car","automobile")` | â€” |
| `.save()` / `.load()` | æ¨¡å‹å„²å­˜ / è¼‰å…¥ | â€” | `.load("model.model")` |
| `.wv.save_word2vec_format()` | è¼¸å‡ºç‚º word2vec æ ¼å¼ | â€” | â€” |

---

## ä¸ƒã€æ–‡ä»¶ç›¸ä¼¼åº¦èˆ‡æª¢ç´¢ï¼ˆDocument Similarityï¼‰

| æ¨¡çµ„ / é¡åˆ¥ | åŠŸèƒ½èªªæ˜ | ç¯„ä¾‹ | å‚™è¨» |
|---------------|------------|-------|------|
| `SparseMatrixSimilarity()` | ç¨€ç–çŸ©é™£ç›¸ä¼¼åº¦è¨ˆç®— | `index = SparseMatrixSimilarity(tfidf_corpus, num_features=1000)` | å¿«é€Ÿ |
| `MatrixSimilarity()` | ç¨ å¯†çŸ©é™£ç›¸ä¼¼åº¦ | â€” | å°å‹èªæ–™ |
| `Similarity()` | è‡ªå‹•ç´¢å¼• + ç›¸ä¼¼åº¦æŸ¥è©¢ | `index = Similarity(None, corpus_tfidf, num_features=500)` | å¯å„²å­˜ |
| `.get_similarities()` | å›å‚³ç›¸ä¼¼åº¦åˆ†æ•¸ | â€” | â€” |
| `.save()` / `.load()` | å„²å­˜ç´¢å¼• | â€” | â€” |

---

## å…«ã€é è¨“ç·´æ¨¡å‹ä¸‹è¼‰ï¼ˆPretrained Modelsï¼‰

| å‡½æ•¸åç¨± | åŠŸèƒ½èªªæ˜ | ç¯„ä¾‹ | å‚™è¨» |
|------------|------------|-------|------|
| `api.load()` | è¼‰å…¥é è¨“ç·´è©å‘é‡ | `glove = api.load("glove-wiki-gigaword-100")` | è‡ªå‹•ä¸‹è¼‰ |
| `glove.most_similar("king")` | æŸ¥è©¢ç›¸ä¼¼è© | â€” | â€” |
| æ”¯æ´æ¨¡å‹ | `"word2vec-google-news-300"`, `"fasttext-wiki-news-subwords-300"`, `"glove-twitter-25"` | â€” | â€” |

---

## ä¹ã€ç›¸ä¼¼åº¦èˆ‡è·é›¢è¨ˆç®—ï¼ˆSimilarity Metricsï¼‰

| å‡½æ•¸åç¨± | åŠŸèƒ½èªªæ˜ | ç¯„ä¾‹ | å‚™è¨» |
|------------|------------|-------|------|
| `similarity()` | è¨ˆç®—å…©è©ç›¸ä¼¼åº¦ | `model.wv.similarity("car", "truck")` | â€” |
| `n_similarity()` | å¤šè©é›†åˆç›¸ä¼¼åº¦ | `model.wv.n_similarity(['man'], ['king'])` | â€” |
| `doesnt_match()` | æ‰¾å‡ºä¸ç›¸é—œè© | `model.wv.doesnt_match("breakfast cereal dinner lunch".split())` | â€” |
| `most_similar_cosmul()` | é¤˜å¼¦ç›¸ä¼¼åº¦ï¼ˆcosmulï¼‰ | `model.wv.most_similar_cosmul(positive=["woman", "king"], negative=["man"])` | â€” |

---

## åã€å¯¦ç”¨å·¥å…·èˆ‡åºåˆ—åŒ–ï¼ˆUtilitiesï¼‰

| å‡½æ•¸ / æ–¹æ³• | åŠŸèƒ½èªªæ˜ | ç¯„ä¾‹ | å‚™è¨» |
|---------------|------------|-------|------|
| `save()` / `load()` | æ¨¡å‹å„²å­˜ / è¼‰å…¥ | `model.save("model.model")` | â€” |
| `corpora.MmCorpus.serialize()` | å„²å­˜èªæ–™ | â€” | â€” |
| `utils.simple_preprocess()` | å¿«é€Ÿåˆ†è© | â€” | NLP å‰è™•ç† |
| `logger` | è¨­å®šè¨“ç·´æ—¥èªŒ | `import logging; logging.basicConfig(format='...')` | â€” |

---



âœ… **å…¸å‹ç¯„ä¾‹ï¼šWord2Vec è¨“ç·´èˆ‡ç›¸ä¼¼åº¦æŸ¥è©¢**

```python
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess

sentences = [
    simple_preprocess("Deep learning improves NLP models"),
    simple_preprocess("Natural language processing is powerful"),
    simple_preprocess("Word embeddings capture semantic meaning")
]

model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)
model.save("word2vec.model")

print(model.wv.most_similar("language"))
