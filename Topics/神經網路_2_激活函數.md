# 神經網路常用的激活函數
```python
"""
Activation Functions Visualization
-----------------------------------
包含:
- Sigmoid
- Tanh
- ReLU
- Leaky ReLU
- Swish
- GELU
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.special import erf  # 用於 GELU 計算

# === 定義常見 Activation Functions ===

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

def relu(x):
    return np.maximum(0, x)

def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

def swish(x):
    return x * sigmoid(x)

def gelu(x):
    # Gaussian Error Linear Unit (GELU)
    return 0.5 * x * (1 + erf(x / np.sqrt(2)))


# === 主程式 ===
x = np.linspace(-6, 6, 500)
plt.figure(figsize=(10, 7))

# 繪製各種函數
plt.plot(x, sigmoid(x), label="Sigmoid", color='blue', linewidth=2)
plt.plot(x, tanh(x), label="Tanh", color='purple', linewidth=2)
plt.plot(x, relu(x), label="ReLU", color='red', linewidth=2)
plt.plot(x, leaky_relu(x), label="Leaky ReLU", color='orange', linewidth=2)
plt.plot(x, swish(x), label="Swish", color='green', linewidth=2)
plt.plot(x, gelu(x), label="GELU", color='brown', linewidth=2)

# 美化圖表
plt.title("Activation Functions Comparison", fontsize=16)
plt.xlabel("x", fontsize=12)
plt.ylabel("f(x)", fontsize=12)
plt.grid(True, linestyle="--", alpha=0.5)
plt.legend(loc="upper left", fontsize=11)
plt.axhline(0, color='black', linewidth=1)
plt.axvline(0, color='black', linewidth=1)
plt.tight_layout()
plt.show()

```
# 神經網路常用的激活函數與其導數
```python
"""
Activation Functions & Derivatives Visualization
-------------------------------------------------
包含:
- Sigmoid
- Tanh
- ReLU
- Leaky ReLU
- Swish
- GELU
顯示 f(x) 與 f'(x) 曲線
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.special import erf

# === 定義 Activation Functions 及其導數 ===

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def d_sigmoid(x):
    s = sigmoid(x)
    return s * (1 - s)

def tanh(x):
    return np.tanh(x)

def d_tanh(x):
    return 1 - np.tanh(x) ** 2

def relu(x):
    return np.maximum(0, x)

def d_relu(x):
    return np.where(x > 0, 1, 0)

def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

def d_leaky_relu(x, alpha=0.01):
    return np.where(x > 0, 1, alpha)

def swish(x):
    s = sigmoid(x)
    return x * s

def d_swish(x):
    s = sigmoid(x)
    return s + x * s * (1 - s)

def gelu(x):
    return 0.5 * x * (1 + erf(x / np.sqrt(2)))

def d_gelu(x):
    # GELU 導數近似式（以高斯分布近似）
    phi = np.exp(-x**2 / 2) / np.sqrt(2 * np.pi)
    return 0.5 * (1 + erf(x / np.sqrt(2))) + 0.5 * x * phi


# === 主程式 ===
x = np.linspace(-6, 6, 600)

# 定義要繪製的函數與導數
functions = {
    "Sigmoid": (sigmoid, d_sigmoid),
    "Tanh": (tanh, d_tanh),
    "ReLU": (relu, d_relu),
    "Leaky ReLU": (leaky_relu, d_leaky_relu),
    "Swish": (swish, d_swish),
    "GELU": (gelu, d_gelu),
}

# === 繪製 f(x) 與 f'(x) ===
plt.figure(figsize=(12, 12))

for i, (name, (f, df)) in enumerate(functions.items()):
    plt.subplot(3, 2, i + 1)
    plt.plot(x, f(x), label=f"{name}", color='blue', linewidth=2)
    plt.plot(x, df(x), label=f"{name} derivative", color='red', linestyle='--', linewidth=2)
    plt.title(f"{name} and its Derivative", fontsize=13)
    plt.xlabel("x")
    plt.ylabel("f(x), f'(x)")
    plt.legend(loc="best", fontsize=9)
    plt.grid(True, linestyle="--", alpha=0.4)
    plt.axhline(0, color='black', linewidth=1)
    plt.axvline(0, color='black', linewidth=1)

plt.tight_layout()
plt.show()
```
